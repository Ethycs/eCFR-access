The most notable thing about this project is the requirement for a server with api that leads to treatment of the data.
In the case where we pull from large datasources, it is not a good pattern to write a secondary server with apis. 
The reason for this is we duplicate work from the originating server which already provides an api.
A microservice architecture or even a juypter notebook would be fine

Second this dataset is huge, within the 4 hours recommended to work this it is unclear what analysis is needed given we unsure of our customer.
Are we a lawyer scouring the regulations for loopholes? Are we trying to condense but a preserve the meaning of regulations? 